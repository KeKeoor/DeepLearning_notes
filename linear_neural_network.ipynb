{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "*2024/10/29*\n",
    "***\n",
    "# 线性神经网络\n",
    "## 线性回归\n",
    "1. 线性模型\n",
    "    线性模型中，假设输入包含d个特征，预测结果$\\hat{y}$表示为\n",
    "\n",
    "    ${\\hat{y}} = w_1x_1 + \\cdot\\cdot\\cdot + w_dx_d + b$\n",
    "\n",
    "    将所有特征放到向量${\\bf{\\it{x}}}\\in\\Bbb{R}^d$中，将所有权重放到向量${\\bf{\\it{w}}}\\in\\Bbb{R}^d$中，使用点积的形式表示模型：\n",
    "\n",
    "    ${\\hat{y}} = {\\bf{\\it{w}}}^{\\rm{T}}{\\bf{\\it{x}}} + b $\n",
    "\n",
    "    向量${\\bf{\\it{x}}}$是单个数据样本的特征，使用矩阵${\\bf{\\it{X}}}\\in{\\Bbb{R}}^{n\\times{d}}$来表示数据集的$n$个样本。其中，${\\bf{\\it{X}}}$的行代表样本，列代表特征。\n",
    "\n",
    "    因此，预测值${\\bf{\\it{\\hat{y}}}}\\in{\\Bbb{R}}^d$可以表示为：\n",
    "\n",
    "    ${\\bf{\\it{\\hat{y}}}} = {\\bf{\\it{X}}}{\\bf{\\it{w}}} + b$\n",
    "\n",
    "    训练的过程就是寻找一组权重向量${\\bf{\\it{w}}}$和偏置$b$，使得预测值和真实值之间的误差尽可能小。\n",
    "\n",
    "2. 损失函数\n",
    "    损失函数（loss function）用来量化预测值和真实值之间的差距。一般选用非负数作为损失，且数值越小表示损失越小。回归中，最常用的损失函数是均方误差（mean squared error, MSE）：\n",
    "\n",
    "    $l^{(i)}({\\bf{\\it{w}}}, b) = \\frac{1}{2}({\\hat{y}}^{(i)} - y^{(i)})^2$\n",
    "\n",
    "    $L({\\bf{\\it{w}}}, b) = \\frac{1}{n}\\sum_{i=1}^{n}l^{(i)}({\\bf{\\it{w}}}, b) = \\frac{1}{n}\\sum_{i=1}^{n}{\\frac{1}{2}}({\\bf{\\it{w}}}^{\\rm{T}}{\\bf{\\it{x}}}^{(i)} + b - y^{(i)})^2$\n",
    "\n",
    "3. 随机梯度下降\n",
    "    梯度下降（gradient descent）通过不断再损失函数递减的方向上更新参数来降低误差。因为要遍历整个数据集，执行可能很慢，因此通常在每次需要计算更新的适合随机抽取一小批样本，称之为小批量随机梯度下降（mini-batch stochastic gradient descent）。\n",
    "\n",
    "    $({\\bf{\\it{w}}}, b) \\gets ({\\bf{\\it{w}}}, b) - \\frac{\\eta}{|B|}\\sum_{{i}\\in{B}}{\\partial_{({\\bf{\\it{w}}}, b)}{l^{(i)}}}({\\bf{\\it{w}}}, b)$\n",
    "\n",
    "    $|B|$表示每个小批量中的样本数，$\\eta$表示学习率（learning rate）。它们的值通常是预先手动指定，而不是训练得到的，称之为超参数（hyperparameter）。\n",
    "\n",
    "    泛化误差（generalization error）是模型在新样本上表现的好坏，而不是在训练集上的表现。为了降低泛化误差，我们通常会在训练集上训练模型，然后在测试集上评估模型的性能。\n",
    "\n",
    "4. <font color=Red>正态分布与平方损失</font>\n",
    "    假设观测中包含的噪声服从正态分布：\n",
    "\n",
    "    $y = {\\bf{\\it{w}}}^{\\rm{T}}{\\bf{\\it{x}}} + b + \\epsilon$\n",
    "\n",
    "    其中，$\\epsilon \\sim N(0, \\sigma^2)$。\n",
    "\n",
    "    因此，给定x观测到特定y的似然（likelihood）:\n",
    "\n",
    "    $P(y|{\\bf{\\it{x}}}) = \\frac{1}{\\sqrt{2{\\pi}{\\sigma}^2}}exp(-\\frac{1}{2{\\sigma}^2}(y-{\\bf{\\it{w}}}^{\\rm{T}}{\\bf{\\it{x}}}-b)^2)$\n",
    "\n",
    "    根据极大似然估计（maximum likelihood estimation），参数${\\bf{\\it{w}}}$和$b$的最优值是使整个数据集的似然最大的值：\n",
    "\n",
    "    $P({\\bf{\\it{y}}}|{\\bf{\\it{X}}}) = \\prod_{i=1}^{n}p(y^{(i)}|{\\bf{\\it{x}}}^{(i)})$\n",
    "\n",
    "5. 神经网络\n",
    "    将线性回归模型描述为一个神经网络，该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值。\n",
    "\n",
    "    ![线性回归是一个单层神经网络。](./img/singleneuron.svg)\n",
    "\n",
    "## 线性回归的实现\n",
    "1. 生成数据集\n",
    "    生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵${\\bf{\\it{X}}}\\in{\\Bbb{R}}^{1000\\times{2}}$。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cxm_pytorch",
   "language": "python",
   "display_name": "CXM_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}